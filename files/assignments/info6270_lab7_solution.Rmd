---
title: "INFO6270 - Lab 7"
author: "Philippe Mongeon"
date: ""
output: 
  html_document:
    toc: true 
    toc_float:       
      collapsed: false
    toc_depth: 2
    number_sections: false 
    theme: lumen
    highlight: tango
---

# Introduction

In this lab you will perform some text-mining and topic modelling using the same abstracts dataset that was used for the example in the course website, but choosing different journals from the dataset. Each of the empty code chunks and questions below are worth .5 pts. There are 9 empty chunks and 2 questions, so the last one is a bonus.

# Setup

## Load libraries

```{r message=FALSE, warning=FALSE}
library(tidyverse) # for all the tidyverse goodies
library(kableExtra) # If you feel like printing pretty tables
library(tidytext) # for all the tidytext goodies
library(textstem) # for the lemmatization and the stemming functions
library(topicmodels) # for topic modelling
```

## Load abstract dataset

```{r message=FALSE, warning=FALSE}
abstracts <- read_tsv("https://pmongeon.github.io/info6270/files/abstracts.txt")
```

## Use filter() to select between 2 and 4 journals

Note: Options are E, F, G, H, I, J. Do not choose A, B, C, D since they were used for the examples in the course website.

```{r}
abstracts <- abstracts %>% 
  filter(journal %in% c("E","F","H"))
```

# Creating you tidy text tibble

## Tokenize the article abstract by word

```{r}
abstracts <- abstracts %>% 
  unnest_tokens(output = word,
               input = abstract,
               token = "words")
```

## Remove stop words from your tokens

```{r}
abstracts <- abstracts %>% 
  anti_join(stop_words, by="word")
```

## Lemmatize the remaining tokens

```{r}
abstracts <- abstracts %>% 
  mutate(word = lemmatize_words(word))
```

# Calculate token frequency for each journals

**Note:** I'm saving this into a new object because I want to keep my tibble with the tokens for each abstracts in my environment

```{r}
journals_words <- abstracts %>% 
  group_by(journal, word) %>% 
  summarize(n = n())
```

Show, for each journal the ten most frequent tokens, along with their total frequency. You have two options. You can choose to do it with tables or with graphs.

```{r}
journals_words %>% 
  group_by(journal) %>% 
  arrange(-n) %>% 
  slice(1:10) %>% 
  ggplot() + 
  aes(x=n, y=reorder(word,n)) +
  geom_col() +
  facet_wrap(facet = ~journal, 
             ncol=3, 
             scale="free") +
  theme_minimal() +
  labs(y = "Word", x="Frequency") +
  scale_y_reordered()
```

# Topic model

## Convert your tibble into a document-term matrix

Name your document-term matrix abstracts_dtm so that the next bit of code works.

```{r}
#first I calculate the term frequency for each abstract
abstracts_words <- abstracts %>% 
  group_by(id, word) %>% 
  summarize(n = n())

abstracts_dtm <- abstracts_words %>% 
  cast_dtm(document = id, 
           term = word, 
           value = n) 
```

## Generate your topic models with LDA

You can use the code below, and use your document-term matrix object as input and change the value of k to the number of topics that you want.

```{r}
abstracts_lda <- abstracts_dtm %>% 
  LDA(k = 3, # I choose three because I have 3 journals and I think this makes sense
      control = list(seed = 1234)) # Just leave this on as is
```

## Top tokens per topic

```{r}
# This codes creates a tibble with the beta of each token for each topic
topic_token_beta <- abstracts_lda %>% 
  tidy(matrix = "beta")
```

Use tables or graphs to show, for each topics, the 10 tokens with the highest beta.

```{r}
topic_token_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot() +
  aes(x=beta, y=term) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  scale_y_reordered()
```

## Associate documents to topics

Create a tibble with the gamma of each abstract-topic combination

```{r}
# This codes creates a tibble with the gamma of each document-topic combination
abstracts_topics <- abstracts_lda %>% 
  tidy(matrix = "gamma")
```

Create a tibble called journal_topic that contains for each, journal, the average gamma for each topic. Note that your abstracts_topics tibble doesn't have the journal column, so that it needs to be brought in from the original dataset with the inner_join function. Then you'll need to group the data by journal and topic and use summarize to calculate the mean(gamma) for each group.

```{r}
journal_topic <- abstracts_topics %>% 
  rename(id = document) %>% 
  mutate(id = as.numeric(id)) %>% 
  inner_join(abstracts, by="id") %>% 
  group_by(journal, topic) %>% 
  summarize(mean_gamma = mean(gamma))
```

Use ggplot to make a series of graphs (use facet_wrap()) showing the distribution of mean gamma across topics for each journal.

```{r}
journal_topic %>% 
  group_by(journal) %>%
  arrange(journal, topic) %>% 
  ggplot() + 
  aes(x=mean_gamma, y=reorder(topic, mean_gamma)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~journal, ncol = 3, scales = "free") +
  labs(y="Topic")

```

Which of your topics is most strongly associated which each journals (e.g., journal E is most strongly related to topic 1, journal F is most strongly associated to topic 2, etc.)?

**Journal E was associated to topic 1, Journal F to topic 2, and journal H to topic 3.**

# Bonus question

Can you tell which journals from this list your selected journals are?

-   American Journal of International Law

-   Food Security

-   Journal of Business Economics and Management **(F)**

-   Journal of Child Psychology and Psychiatry

-   Journal of Mathematics (**H)**

-   Journal of the Association for Information Science and Technology **(E)**

-   Nature Astronomy

-   Organic & Biomolecular Chemistry

-   Public Health

-   Solar Energy
