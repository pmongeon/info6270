[["chapter-11---linear-regression.html", "Chapter 11 - Linear regression Introduction Linear regression Fitting the regression line on data Building the regression model Homework", " Chapter 11 - Linear regression Introduction In the previous chapter, we learned that regression is a method used to determine the relationship between a dependent variable (the variable we want to predict) and one or more independent variables (the predictors available to make the prediction). In the last chapter, we used logistic regression to predict a dichotomous variable. In this chapter we will learn how to use linear regression to predict a continuous dependent variable. Linear regression Linear regression uses a straight line to model the relationship between categorical or numerical predictors (independent variables) and a numerical predicted value (dependent variable). for a single predictor variable, the formula for the prediction is: \\[ y = intercept + slope × x \\] In statistical terms, the same formula is written like this: \\[ y = _0 + x \\] And if we have multiple independent variables, the formula becomes: \\[ y = _0 + _1x_1 + _2x_2 .... _nx_n \\] To determine how well the model fits the data (how well does x predict y. the linear model uses the square of the residuals (r2). The residuals are the difference between the predicted values and the real data, measured by the vertical distance between the line and the data points. Heres a figure from Rhys (2020) to help you visualize this. We can see in this figure that the intercept is where the line crosses the y-axis. The slope is calculate by dividing the difference in the predicted value of y by the difference in the value of x. When working with categorical predictors, the intercept is the mean value of the base category and the slope is the difference between the means of each categories. Heres an example taken again from Rhys (2020). Example: predicting flight delays Lets use on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013 to try to build a model that will predicted delayed arrival. For this we will use the flights dataset included in the nycflights13 package. We will consider the following variables in our model: origin: airport of departure (JFK, LGA, EWR) carrier (we will only compare United Airlines - UA, and American Airlines - AA) distance: flight distance in miles. dep_delay: delay of departure in minutes arr_delay: delay of arrival in minutes (this is our independent variable) library(nycflights13) data &lt;- flights %&gt;% filter(carrier %in% c(&quot;UA&quot;, &quot;AA&quot;)) %&gt;% select(origin, carrier, distance, dep_delay, arr_delay) %&gt;% mutate(origin = as_factor(origin), carrier = as_factor(carrier)) %&gt;% drop_na() Fitting the regression line on data Continuous dependent variables data %&gt;% ggplot() + aes(dep_delay, arr_delay) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=F) Categorical dependent variables Carrier In order to visualize the trend line with geom_smooth, we need to convert our factors into numerical variables. data %&gt;% mutate(carrier = as.numeric(carrier)) %&gt;% ggplot() + aes(carrier, arr_delay) + geom_point() + geom_smooth(method=&quot;lm&quot;) Origin Since origin has three levels (EWR, LGA and JFK), we want to plot choose a base level and compare each other level to this one. We first compare level 1 and 2 data %&gt;% mutate(origin = as.numeric(origin)) %&gt;% filter(origin %in% c(1,2)) %&gt;% ggplot() + aes(origin, arr_delay) + geom_point() + geom_smooth(method=&quot;lm&quot;)  then we compare level 1 and 3. data %&gt;% mutate(origin = as.numeric(origin)) %&gt;% filter(origin %in% c(1,3)) %&gt;% ggplot() + aes(origin, arr_delay) + geom_point() + geom_smooth(method=&quot;lm&quot;) Building the regression model The process to build the model is exactly the same as the one we used for the logistic regression in the previous chapter. In fact, the process is simpler here because we do not need to convert the coefficient into odds ratios to make them easier to interpret. Lets use the lm() function to build the model that predict delay at arrival based on the distance of the flight, the carrier and the origin (well leave the delay of departure out of the model for now). model &lt;- lm(arr_delay ~ distance + carrier + origin, data = data) summary(model) ## ## Call: ## lm(formula = arr_delay ~ distance + carrier + origin, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -77.86 -21.95 -9.71 7.89 1006.23 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6299672 0.3577348 12.942 &lt; 2e-16 *** ## distance -0.0007208 0.0002005 -3.596 0.000324 *** ## carrierAA -3.6699113 0.3907133 -9.393 &lt; 2e-16 *** ## originLGA -0.7238011 0.4052653 -1.786 0.074104 . ## originJFK 1.6725192 0.4638028 3.606 0.000311 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 41.53 on 89724 degrees of freedom ## Multiple R-squared: 0.001708, Adjusted R-squared: 0.001663 ## F-statistic: 38.37 on 4 and 89724 DF, p-value: &lt; 2.2e-16 The estimate coefficient represents the slope of the linear trend line for each predictor, so we can plug these values into our linear equation. \\[ ArrDelay = 4.63 - 0.00distance - 3.67AA - 0.73LGA + 1.67JFK \\] We can see that most coefficient are statistically significant, which appears to indicate that they are good predictors, but lets hold on for a minute before drawing too hasty conclusions. Look at the Adjusted R-squared (r2). It as a value of 0.001663, which is extremely small and indicate that the model explains less then 1% of the variance in delays. In other words, our model does not at all allow us to make predictions about delays. Beware of too large sample Statistically significant predictors in a model with low predictive value mostly occur when our data set or sample is too large. Whats a good sample size? a good rule of thumb is 10% of the total observations, with at least 10 observations per variable in the model but no more than 1000 observations in total. Lets do this again with a sample of 500 observations. model &lt;- lm(arr_delay ~ distance + carrier + origin, data = sample_n(data, 500)) summary(model) ## ## Call: ## lm(formula = arr_delay ~ distance + carrier + origin, data = sample_n(data, ## 500)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -59.471 -20.395 -8.375 9.467 310.470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.649806 4.108669 1.375 0.16972 ## distance -0.002925 0.002271 -1.288 0.19824 ## carrierAA -13.236864 4.593706 -2.882 0.00413 ** ## originLGA 4.143979 4.695726 0.883 0.37793 ## originJFK 14.486048 5.596356 2.588 0.00992 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 36.08 on 495 degrees of freedom ## Multiple R-squared: 0.02082, Adjusted R-squared: 0.0129 ## F-statistic: 2.631 on 4 and 495 DF, p-value: 0.03372 We see that the model still does a terrible job at predicting arrival delays, and that none of the predictors are statistically significant (at the p &lt; 0.05 level). Adding departure delay to the model Finally, lets ad the departure delay to the model. Weve seen in the figure above in which we plotted the arrival delay against the departure delay, that our data points seemed to follow our trend line, and so we can expect that adding this predictor will improve our model. model &lt;- lm(arr_delay ~ distance + carrier + origin + dep_delay, data = sample_n(data, 500)) summary(model) ## ## Call: ## lm(formula = arr_delay ~ distance + carrier + origin + dep_delay, ## data = sample_n(data, 500)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.724 -12.588 -0.944 9.846 119.725 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.361112 2.216456 -4.223 2.86e-05 *** ## distance -0.000598 0.001227 -0.487 0.626 ## carrierAA -0.326978 2.437660 -0.134 0.893 ## originLGA 0.853386 2.586331 0.330 0.742 ## originJFK 2.476063 2.847907 0.869 0.385 ## dep_delay 1.052191 0.023771 44.264 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.49 on 494 degrees of freedom ## Multiple R-squared: 0.8001, Adjusted R-squared: 0.798 ## F-statistic: 395.4 on 5 and 494 DF, p-value: &lt; 2.2e-16 We can see that when we consider the delay in the departure, we can more accurately predict the delay at arrival, with about 80% of the variance explained by our model! Homework For this weeks lab, your task is to build a linear regression model on a dataset of your choice. I encourage you to use the dataset that you will be using for your individual project, if it contains a numerical variable that you can try to predict using other variables in your dataset. A template with a dataset will be available very shortly. References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
